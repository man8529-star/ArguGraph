{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c50ded3-4ab0-408f-be0c-7e82e5fd2718",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d675ab44-5afc-41da-8b33-23d6e25a262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DRAFT_2_14.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dc4a7-46ad-4ece-995a-744c552c01ce",
   "metadata": {},
   "source": [
    "## Section 1: Structural Representation\n",
    "\n",
    "Essays were segmented at the independent clause level and annotated with gold parent labels. Parent labels encode hierarchical structure (e.g., SP1.R1.R1).  \n",
    "These prefix-based encodings allow reconstruction of hierarchical argument trees.\n",
    "\n",
    "Why Clause-Level Segmentation?\n",
    "\n",
    "Long sentences often contain multiple reasoning steps. Segmenting at the independent clause level allows us to:\n",
    "- Model justification chains more precisely\n",
    "- Capture coordination between reasons\n",
    "- Represent hierarchical reasoning more accurately\n",
    "\n",
    "This provides a fine-grained structural view of how arguments are built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f51fb-d13d-45fe-a493-25d404b31579",
   "metadata": {},
   "source": [
    "## Section 2: Construct Structural Trees\n",
    "This section reconstructs hierarchical argument trees from prefix-based structural labels and inserts virtual intermediate nodes where necessary.\n",
    "\n",
    "After reconstruction, each essay is represented as a structural tree where each row corresponds to one node (either a sentence node or a virtual coordination node).\n",
    "\n",
    "For the purposes of this project, the following structural fields are essential:\n",
    "- node_id – Unique identifier of the node\n",
    "- parent_node_id – Immediate parent in the argument tree\n",
    "- hier_path – Prefix-based structural path (e.g., SP1.R1.R1)\n",
    "- depth – Hierarchical level in the reasoning chain\n",
    "- num_children – Number of direct descendants\n",
    "- node_type – Indicates whether the node is a SENTENCE or VIRTUAL\n",
    "\n",
    "These fields fully define the argument tree structure and are used for:\n",
    "- Building the pairwise parent–child dataset\n",
    "- Computing structural complexity metrics\n",
    "- Evaluating reconstructed trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3794c82-cdd5-4fe2-ad7d-22a38424f9a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_node_id</th>\n",
       "      <th>hier_path</th>\n",
       "      <th>depth_y</th>\n",
       "      <th>num_children</th>\n",
       "      <th>node_type</th>\n",
       "      <th>segment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Intro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>The question is can any obstacle or disadvanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SP1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>Yes it can,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SP1.R1a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>because even in the story he couldn't get in b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SP1.R1b</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>and the guy tells him if it's a drama, \"Smash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SP1.R2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>But when you think about it, if a girl was gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>But that's not the point but you can change a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008_1_2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SP1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>Yes, obstacles and disadvantages can be turned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008_1_2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SP1.R1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>because you will know how to over come obstacl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008_1_2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SP1.R2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>You can help other people with obstacles and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008_1_2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SP1.R3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>You also can learn from them it can also help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008_2_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Intro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>I live in a house that every body in it came f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2008_2_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SP1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>I think she ment that what ever is the difficu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008_2_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SP1.R1a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>For an example, I grow up in place that full w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2008_2_1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SP1.R1b</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>and one time some body try to convinse me to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008_2_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SP1.R1c</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SENTENCE</td>\n",
       "      <td>And smoking it very bad thing.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    essay_id  node_id  parent_node_id hier_path  depth_y  num_children  \\\n",
       "0   2008_1_1      1.0             NaN     Intro      0.0           0.0   \n",
       "1   2008_1_1      2.0             NaN       SP1      0.0           3.0   \n",
       "2   2008_1_1      3.0             2.0   SP1.R1a      1.0           0.0   \n",
       "3   2008_1_1      4.0             2.0   SP1.R1b      1.0           0.0   \n",
       "4   2008_1_1      5.0             2.0    SP1.R2      1.0           0.0   \n",
       "5   2008_1_1      6.0             NaN         C      0.0           0.0   \n",
       "6   2008_1_2      1.0             NaN       SP1      0.0           3.0   \n",
       "7   2008_1_2      2.0             1.0    SP1.R1      1.0           0.0   \n",
       "8   2008_1_2      3.0             1.0    SP1.R2      1.0           0.0   \n",
       "9   2008_1_2      4.0             1.0    SP1.R3      1.0           0.0   \n",
       "10  2008_2_1      1.0             NaN     Intro      0.0           0.0   \n",
       "11  2008_2_1      2.0             NaN       SP1      0.0           6.0   \n",
       "12  2008_2_1      3.0             2.0   SP1.R1a      1.0           0.0   \n",
       "13  2008_2_1      4.0             2.0   SP1.R1b      1.0           0.0   \n",
       "14  2008_2_1      5.0             2.0   SP1.R1c      1.0           0.0   \n",
       "\n",
       "   node_type                                       segment_text  \n",
       "0   SENTENCE  The question is can any obstacle or disadvanta...  \n",
       "1   SENTENCE                                        Yes it can,  \n",
       "2   SENTENCE  because even in the story he couldn't get in b...  \n",
       "3   SENTENCE  and the guy tells him if it's a drama, \"Smash ...  \n",
       "4   SENTENCE  But when you think about it, if a girl was gui...  \n",
       "5   SENTENCE  But that's not the point but you can change a ...  \n",
       "6   SENTENCE  Yes, obstacles and disadvantages can be turned...  \n",
       "7   SENTENCE  because you will know how to over come obstacl...  \n",
       "8   SENTENCE  You can help other people with obstacles and d...  \n",
       "9   SENTENCE  You also can learn from them it can also help ...  \n",
       "10  SENTENCE  I live in a house that every body in it came f...  \n",
       "11  SENTENCE  I think she ment that what ever is the difficu...  \n",
       "12  SENTENCE  For an example, I grow up in place that full w...  \n",
       "13  SENTENCE  and one time some body try to convinse me to s...  \n",
       "14  SENTENCE                     And smoking it very bad thing.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstructed Tree Format and Node Schema\n",
    "import pandas as pd\n",
    "\n",
    "structure_df = pd.read_excel(\"FULL_ARGUMENT_STRUCTURE.xlsx\")\n",
    "\n",
    "# Select only the columns relevant for structural illustration\n",
    "cols_to_show = [\n",
    "    \"essay_id\",\n",
    "    \"node_id\",\n",
    "    \"parent_node_id\",\n",
    "    \"hier_path\",\n",
    "    \"depth_y\",          # or \"depth\" depending on your column name\n",
    "    \"num_children\",\n",
    "    \"node_type\",\n",
    "    \"segment_text\"      # <-- this shows the actual sentence\n",
    "]\n",
    "\n",
    "structure_df_display = structure_df[cols_to_show]\n",
    "\n",
    "print(\"Total nodes:\", len(structure_df_display))\n",
    "structure_df_display.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c44f7-73f9-48ba-85e9-2cab6270fed2",
   "metadata": {},
   "source": [
    "## Section 3: Parent–Child Prediction Dataset\n",
    "\n",
    "In argument trees, some propositions function independently, while others form coordinated groups that jointly support a higher-level proposition. Modeling such structures with a binary parent–child classification setup can be challenging. The classification model assumes that each proposition attaches to a single parent, which may not fully capture coordinated argument clusters.\n",
    "\n",
    "To examine this structural issue, we construct two versions of the pairwise dataset:\n",
    "- A collapsed version, where the tree is reduced to independent parent–child pairs.\n",
    "- An expanded version, where coordination is explicitly represented using virtual nodes.\n",
    "\n",
    "This allows us to evaluate how structural representation affects parent selection performance.\n",
    "\n",
    "\n",
    "\n",
    "### 3.1 Collapsed Pairwise Dataset\n",
    "To train the parent selection model, each essay's argument tree is converted into multiple parent-child pairs. \n",
    "\n",
    "For each proposition (\"the child\"):\n",
    "\n",
    "- The correct parent forms a positive example.\n",
    "- Earlier propositions are treated possilbe candidate parents and sampled as negative examples.\n",
    "\n",
    "Each pair includes:\n",
    "\n",
    "- Child text\n",
    "- Candidate parent text\n",
    "- Structural features:\n",
    "  - Segment distance: How far apart the segments are\n",
    "  - Same-paragraph indicator\n",
    "  - Paragraph distance\n",
    "\n",
    "This formulation trains the model to answer a simple question: \n",
    "\"Is this the correct parent for this sentence?\"\n",
    "\n",
    "At prediction time, the model scores all candidate parents to a given child and selects the highest-scoring one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "474b1adc-d191-434f-91b8-a37edd0d3a63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: dataset_for_parent_pairwise_models.csv\n",
      "Total rows: 1796\n",
      "Positive ratio: 0.167\n",
      "\n",
      "Label counts:\n",
      "y\n",
      "0    1496\n",
      "1     300\n",
      "Name: count, dtype: int64\n",
      "test_df exists? False\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BERT Parent-Prediction Dataset Builder\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "INPUT_XLSX = \"FULL_ARGUMENT_STRUCTURE.xlsx\"\n",
    "OUT_PAIRWISE = \"dataset_for_parent_pairwise_models.csv\"\n",
    "\n",
    "KEEP_ROLES = {\"CLAIM\", \"REASON\"}\n",
    "K_NEG = 8\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load Data\n",
    "# ---------------------------------------\n",
    "\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "\n",
    "# Keep only real sentence nodes\n",
    "df = df[df[\"segment_text\"].notna()].copy()\n",
    "\n",
    "# Keep only structural roles (GOLD)\n",
    "df = df[df[\"node_role_y\"].isin(KEEP_ROLES)].copy()\n",
    "\n",
    "# Ensure numeric\n",
    "df[\"segment_id\"] = pd.to_numeric(df[\"segment_id\"], errors=\"coerce\")\n",
    "df[\"parent_node_id\"] = pd.to_numeric(df[\"parent_node_id\"], errors=\"coerce\")\n",
    "df[\"node_id\"] = pd.to_numeric(df[\"node_id\"], errors=\"coerce\")\n",
    "df[\"para_id\"] = pd.to_numeric(df[\"para_id\"], errors=\"coerce\")\n",
    "\n",
    "pairs = []\n",
    "\n",
    "# ============================================\n",
    "# Build Pairwise Dataset\n",
    "# ============================================\n",
    "\n",
    "for essay_id, g in df.groupby(\"essay_id\", sort=False):\n",
    "\n",
    "    g = g.sort_values(\"segment_id\")\n",
    "\n",
    "    # Map node_id → row\n",
    "    rows = {\n",
    "        int(r.node_id): r\n",
    "        for r in g.itertuples()\n",
    "        if not pd.isna(r.node_id)\n",
    "    }\n",
    "\n",
    "    for child in g.itertuples():\n",
    "\n",
    "        child_id = int(child.node_id)\n",
    "        gold_parent = child.parent_node_id\n",
    "\n",
    "        # Must have a real gold parent\n",
    "        if pd.isna(gold_parent):\n",
    "            continue\n",
    "\n",
    "        gold_parent = int(gold_parent)\n",
    "\n",
    "        # Candidate parents = earlier sentences only\n",
    "        candidates = g[g[\"segment_id\"] < child.segment_id]\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "\n",
    "        # =====================================\n",
    "        # POSITIVE EXAMPLE\n",
    "        # =====================================\n",
    "\n",
    "        if gold_parent in rows:\n",
    "\n",
    "            parent_row = rows[gold_parent]\n",
    "\n",
    "            # Structural features\n",
    "            seg_dist = child.segment_id - parent_row.segment_id\n",
    "            same_para = int(child.para_id == parent_row.para_id)\n",
    "            para_dist = child.para_id - parent_row.para_id\n",
    "\n",
    "            pairs.append({\n",
    "                \"essay_id\": essay_id,\n",
    "                \"child_node_id\": child_id,\n",
    "                \"cand_parent_node_id\": gold_parent,\n",
    "                \"y\": 1,\n",
    "\n",
    "                \"child_text\": child.segment_text,\n",
    "                \"parent_text\": parent_row.segment_text,\n",
    "\n",
    "                \"child_role\": child.node_role_y,\n",
    "                \"parent_role\": parent_row.node_role_y,\n",
    "\n",
    "                \"seg_distance\": seg_dist,\n",
    "                \"same_para\": same_para,\n",
    "                \"para_distance\": para_dist,\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            # Parent is virtual → skip\n",
    "            continue\n",
    "\n",
    "        # =====================================\n",
    "        # NEGATIVE EXAMPLES\n",
    "        # =====================================\n",
    "\n",
    "        cand_ids = candidates[\"node_id\"].dropna().astype(int).tolist()\n",
    "        cand_ids = [cid for cid in cand_ids if cid != gold_parent]\n",
    "\n",
    "        if len(cand_ids) == 0:\n",
    "            continue\n",
    "\n",
    "        neg_ids = random.sample(\n",
    "            cand_ids,\n",
    "            min(K_NEG, len(cand_ids))\n",
    "        )\n",
    "\n",
    "        for neg_id in neg_ids:\n",
    "\n",
    "            neg_row = rows.get(int(neg_id))\n",
    "            if neg_row is None:\n",
    "                continue\n",
    "\n",
    "            # Structural features\n",
    "            seg_dist = child.segment_id - neg_row.segment_id\n",
    "            same_para = int(child.para_id == neg_row.para_id)\n",
    "            para_dist = child.para_id - neg_row.para_id\n",
    "\n",
    "            pairs.append({\n",
    "                \"essay_id\": essay_id,\n",
    "                \"child_node_id\": child_id,\n",
    "                \"cand_parent_node_id\": int(neg_id),\n",
    "                \"y\": 0,\n",
    "\n",
    "                \"child_text\": child.segment_text,\n",
    "                \"parent_text\": neg_row.segment_text,\n",
    "\n",
    "                \"child_role\": child.node_role_y,\n",
    "                \"parent_role\": neg_row.node_role_y,\n",
    "\n",
    "                \"seg_distance\": seg_dist,\n",
    "                \"same_para\": same_para,\n",
    "                \"para_distance\": para_dist,\n",
    "            })\n",
    "\n",
    "# ============================================\n",
    "# Save\n",
    "# ============================================\n",
    "\n",
    "pair_df = pd.DataFrame(pairs)\n",
    "pair_df.to_csv(OUT_PAIRWISE, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_PAIRWISE)\n",
    "print(\"Total rows:\", len(pair_df))\n",
    "print(\"Positive ratio:\", round(pair_df[\"y\"].mean(), 4))\n",
    "print(\"\\nLabel counts:\")\n",
    "print(pair_df[\"y\"].value_counts())\n",
    "print(\"test_df exists?\", \"test_df\" in globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054379c-901f-422e-8e93-17814464bbf5",
   "metadata": {},
   "source": [
    "### 3.2 Pairwise Dataset with Virtual Nodes\n",
    "To better handle coordinative nodes, we build a second version of the dataset that includes virtual nodes to represent coordinated argument clusters.\n",
    "\n",
    "The construction process remains the same:\n",
    "- The gold parent forms a positive example.\n",
    "- Earlier nodes (including virtual nodes) are used as candidate parents.\n",
    "- A fixed number of negative candidates are sampled per child.\n",
    "\n",
    "The key difference is that virtual nodes are treated as valid parent candidates. When a candidate parent is virtual, its structural position is preserved and a placeholder token [VIRTUAL_NODE] is used instead of sentence text.\n",
    "\n",
    "This allows the model to learn parent selection in trees that explicitly represent coordination, rather than forcing coordinated propositions to attach separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bde74359-a315-4df7-8e8e-f639403a1704",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: dataset_parent_pairwise_v2_virtual.csv\n",
      "Total rows: 3434\n",
      "Positive ratio: 0.143\n",
      "\n",
      "Label counts:\n",
      "y\n",
      "0    2943\n",
      "1     491\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Fix the pair-wise model: \n",
    "# ============================================\n",
    "# BERT Parent-Prediction Dataset Builder\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "INPUT_XLSX = \"FULL_ARGUMENT_STRUCTURE.xlsx\"\n",
    "OUT_PAIRWISE = \"dataset_parent_pairwise_v2_virtual.csv\"\n",
    "\n",
    "KEEP_ROLES = {\"CLAIM\", \"REASON\"}\n",
    "K_NEG = 8\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load Data\n",
    "# ---------------------------------------\n",
    "\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "\n",
    "# Keep sentence nodes OR virtual nodes\n",
    "df = df[\n",
    "    (df[\"segment_text\"].notna()) |\n",
    "    (df[\"node_type\"] == \"VIRTUAL\")\n",
    "].copy()\n",
    "\n",
    "# Keep CLAIM / REASON OR virtual nodes\n",
    "df = df[\n",
    "    (df[\"node_role_y\"].isin(KEEP_ROLES)) |\n",
    "    (df[\"node_type\"] == \"VIRTUAL\")\n",
    "].copy()\n",
    "\n",
    "# Ensure numeric\n",
    "df[\"segment_id\"] = pd.to_numeric(df[\"segment_id\"], errors=\"coerce\")\n",
    "df[\"parent_node_id\"] = pd.to_numeric(df[\"parent_node_id\"], errors=\"coerce\")\n",
    "df[\"node_id\"] = pd.to_numeric(df[\"node_id\"], errors=\"coerce\")\n",
    "df[\"para_id\"] = pd.to_numeric(df[\"para_id\"], errors=\"coerce\")\n",
    "\n",
    "pairs = []\n",
    "\n",
    "# ============================================\n",
    "# Build Pairwise Dataset\n",
    "# ============================================\n",
    "\n",
    "for essay_id, g in df.groupby(\"essay_id\", sort=False):\n",
    "\n",
    "    g = g.sort_values(by=[\"segment_id\"], na_position=\"first\")\n",
    "\n",
    "    # Map node_id → row\n",
    "    rows = {\n",
    "        int(r.node_id): r\n",
    "        for r in g.itertuples()\n",
    "        if not pd.isna(r.node_id)\n",
    "    }\n",
    "\n",
    "    for child in g.itertuples():\n",
    "\n",
    "        child_id = int(child.node_id)\n",
    "        gold_parent = child.parent_node_id\n",
    "\n",
    "        # Must have a real gold parent\n",
    "        if pd.isna(gold_parent):\n",
    "            continue\n",
    "\n",
    "        gold_parent = int(gold_parent)\n",
    "\n",
    "        # Candidate parents = earlier sentences only\n",
    "        candidates = g[\n",
    "            (g[\"segment_id\"] < child.segment_id) |\n",
    "            (g[\"node_type\"] == \"VIRTUAL\")\n",
    "]\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "\n",
    "        # =====================================\n",
    "        # POSITIVE EXAMPLE\n",
    "        # =====================================\n",
    "\n",
    "        parent_row = rows.get(gold_parent)\n",
    "\n",
    "        if parent_row is None:\n",
    "            continue\n",
    "\n",
    "        # ---- SAFE structural features (virtual-proof) ----\n",
    "        parent_seg = parent_row.segment_id if not pd.isna(parent_row.segment_id) else child.segment_id\n",
    "        parent_para = parent_row.para_id if not pd.isna(parent_row.para_id) else child.para_id\n",
    "\n",
    "        seg_dist = child.segment_id - parent_seg\n",
    "        same_para = int(child.para_id == parent_para)\n",
    "        para_dist = child.para_id - parent_para\n",
    "\n",
    "        pairs.append({\n",
    "            \"essay_id\": essay_id,\n",
    "            \"child_node_id\": child_id,\n",
    "            \"cand_parent_node_id\": gold_parent,\n",
    "            \"y\": 1,\n",
    "\n",
    "            \"child_text\": child.segment_text,\n",
    "            \"parent_text\": parent_row.segment_text if pd.notna(parent_row.segment_text) else \"[VIRTUAL_NODE]\",\n",
    "\n",
    "            \"child_role\": child.node_role_y,\n",
    "            \"parent_role\": parent_row.node_role_y,\n",
    "\n",
    "            \"seg_distance\": seg_dist,\n",
    "            \"same_para\": same_para,\n",
    "            \"para_distance\": para_dist,\n",
    "        })\n",
    "\n",
    "\n",
    "        # =====================================\n",
    "        # NEGATIVE EXAMPLES\n",
    "        # =====================================\n",
    "\n",
    "        cand_ids = candidates[\"node_id\"].dropna().astype(int).tolist()\n",
    "        cand_ids = [cid for cid in cand_ids if cid != gold_parent]\n",
    "\n",
    "        if len(cand_ids) == 0:\n",
    "            continue\n",
    "\n",
    "        neg_ids = random.sample(\n",
    "            cand_ids,\n",
    "            min(K_NEG, len(cand_ids))\n",
    "        )\n",
    "\n",
    "        for neg_id in neg_ids:\n",
    "\n",
    "            neg_row = rows.get(int(neg_id))\n",
    "            if neg_row is None:\n",
    "                continue\n",
    "\n",
    "            # ---- SAFE structural features ----\n",
    "            parent_seg = neg_row.segment_id if not pd.isna(neg_row.segment_id) else child.segment_id\n",
    "            parent_para = neg_row.para_id if not pd.isna(neg_row.para_id) else child.para_id\n",
    "\n",
    "            seg_dist = child.segment_id - parent_seg\n",
    "            same_para = int(child.para_id == parent_para)\n",
    "            para_dist = child.para_id - parent_para\n",
    "\n",
    "            pairs.append({\n",
    "                \"essay_id\": essay_id,\n",
    "                \"child_node_id\": child_id,\n",
    "                \"cand_parent_node_id\": int(neg_id),\n",
    "                \"y\": 0,\n",
    "\n",
    "                \"child_text\": child.segment_text,\n",
    "                \"parent_text\": neg_row.segment_text if pd.notna(neg_row.segment_text) else \"[VIRTUAL_NODE]\",\n",
    "\n",
    "                \"child_role\": child.node_role_y,\n",
    "                \"parent_role\": neg_row.node_role_y,\n",
    "\n",
    "                \"seg_distance\": seg_dist,\n",
    "                \"same_para\": same_para,\n",
    "                \"para_distance\": para_dist,\n",
    "            })\n",
    "# ============================================\n",
    "# Save\n",
    "# ============================================\n",
    "\n",
    "pair_df = pd.DataFrame(pairs)\n",
    "pair_df.to_csv(OUT_PAIRWISE, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_PAIRWISE)\n",
    "print(\"Total rows:\", len(pair_df))\n",
    "print(\"Positive ratio:\", round(pair_df[\"y\"].mean(), 4))\n",
    "print(\"\\nLabel counts:\")\n",
    "print(pair_df[\"y\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d135d-5f0e-435d-92a9-0f01406062b5",
   "metadata": {},
   "source": [
    "## Section 4: Stratifying essays by complexity level\n",
    "We compute structural complexity metrics from each reconstructed argument tree:\n",
    "\n",
    "- Max Depth: longest reasoning chain\n",
    "- Average Parent Distance: how far propositions attach\n",
    "- Cross-Paragraph Links: proportion of cross-paragraph reasoning\n",
    "- Average Candidate Count: attachment ambiguity\n",
    "\n",
    "These features are normalized and aggregated into a composite complexity score. \n",
    "\n",
    "Essays are then partitioned into three groups based on score quantiles:\n",
    "\n",
    "- LOW complexity (bottom third)  \n",
    "- MID complexity (middle third)  \n",
    "- HIGH complexity (top third)  \n",
    "\n",
    "Why Stratify?\n",
    "\n",
    "Structural complexity varies substantially across essays. Without stratification, train/validation/test splits may overrepresent shallow or deep structures.\n",
    "\n",
    "We therefore perform essay-level stratified splitting by complexity group to:\n",
    "\n",
    "- Ensure balanced exposure to shallow and deep reasoning patterns during training  \n",
    "- Prevent distributional skew across splits  \n",
    "- Enable fine-grained performance analysis by structural group  \n",
    "\n",
    "This design allows us to evaluate not only overall accuracy, but also robustness across different levels of argument complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01104584-1182-4b66-8e3d-048e831d243b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>avg_parent_distance</th>\n",
       "      <th>pct_cross_para</th>\n",
       "      <th>avg_candidate_count</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>complexity_score</th>\n",
       "      <th>complexity_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008_1_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008_1_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008_2_1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.144828</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008_2_2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>23</td>\n",
       "      <td>0.442026</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008_3_1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.095618</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  max_depth  avg_parent_distance  pct_cross_para  \\\n",
       "0  2008_1_1        0.0             0.043103        0.000000   \n",
       "1  2008_1_2        0.0             0.043103        0.000000   \n",
       "2  2008_2_1        0.2             0.129310        0.000000   \n",
       "3  2008_2_2        0.6             0.043103        0.333333   \n",
       "4  2008_3_1        0.2             0.057471        0.000000   \n",
       "\n",
       "   avg_candidate_count  num_nodes  complexity_score complexity_level  \n",
       "0             0.000000          4          0.010776              LOW  \n",
       "1             0.000000          4          0.010776              LOW  \n",
       "2             0.250000         10          0.144828              LOW  \n",
       "3             0.791667         23          0.442026             HIGH  \n",
       "4             0.125000          7          0.095618              LOW  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complexity distribution:\n",
      "complexity_level\n",
      "LOW     11\n",
      "HIGH    11\n",
      "MID     10\n",
      "Name: count, dtype: int64\n",
      "Saved: essay_complexity_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 3: Essay-Level Structural Complexity\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "INPUT_XLSX = \"FULL_ARGUMENT_STRUCTURE.xlsx\"\n",
    "\n",
    "# -----------------------------------\n",
    "# Load Data\n",
    "# -----------------------------------\n",
    "\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "\n",
    "# Keep sentence-level structural nodes\n",
    "df = df[df[\"segment_text\"].notna()].copy()\n",
    "df = df[df[\"node_role_y\"].isin({\"CLAIM\", \"REASON\"})].copy()\n",
    "\n",
    "# Ensure numeric\n",
    "df[\"segment_id\"] = pd.to_numeric(df[\"segment_id\"], errors=\"coerce\")\n",
    "df[\"parent_node_id\"] = pd.to_numeric(df[\"parent_node_id\"], errors=\"coerce\")\n",
    "df[\"node_id\"] = pd.to_numeric(df[\"node_id\"], errors=\"coerce\")\n",
    "df[\"para_id\"] = pd.to_numeric(df[\"para_id\"], errors=\"coerce\")\n",
    "df[\"depth\"] = pd.to_numeric(df[\"depth_y\"], errors=\"coerce\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Compute Essay-Level Metrics\n",
    "# -----------------------------------\n",
    "\n",
    "essay_stats = []\n",
    "\n",
    "for essay_id, g in df.groupby(\"essay_id\"):\n",
    "\n",
    "    g = g.sort_values(\"segment_id\")\n",
    "\n",
    "    # 1. Max depth\n",
    "    max_depth = g[\"depth\"].max()\n",
    "\n",
    "    # 2. Avg parent distance\n",
    "    parent_distances = []\n",
    "    node_lookup = {int(r.node_id): r for r in g.itertuples()}\n",
    "\n",
    "    for row in g.itertuples():\n",
    "        if pd.isna(row.parent_node_id):\n",
    "            continue\n",
    "\n",
    "        parent = node_lookup.get(int(row.parent_node_id))\n",
    "        if parent is None:\n",
    "            continue\n",
    "\n",
    "        parent_distances.append(row.segment_id - parent.segment_id)\n",
    "\n",
    "    avg_parent_distance = np.mean(parent_distances) if parent_distances else 0\n",
    "\n",
    "    # 3.  % cross-paragraph links\n",
    "    cross_links = []\n",
    "\n",
    "    for row in g.itertuples():\n",
    "        if pd.isna(row.parent_node_id):\n",
    "            continue\n",
    "\n",
    "        parent = node_lookup.get(int(row.parent_node_id))\n",
    "        if parent is None:\n",
    "            continue\n",
    "\n",
    "        cross_links.append(int(row.para_id != parent.para_id))\n",
    "\n",
    "    pct_cross_para = np.mean(cross_links) if cross_links else 0\n",
    "\n",
    "    # 4. Avg candidate count\n",
    "    candidate_counts = [\n",
    "        len(g[g[\"segment_id\"] < row.segment_id])\n",
    "        for row in g.itertuples()\n",
    "    ]\n",
    "\n",
    "    avg_candidate_count = np.mean(candidate_counts)\n",
    "\n",
    "    essay_stats.append({\n",
    "        \"essay_id\": essay_id,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"avg_parent_distance\": avg_parent_distance,\n",
    "        \"pct_cross_para\": pct_cross_para,\n",
    "        \"avg_candidate_count\": avg_candidate_count,\n",
    "        \"num_nodes\": len(g)\n",
    "    })\n",
    "\n",
    "complexity_df = pd.DataFrame(essay_stats)\n",
    "\n",
    "# -----------------------------------\n",
    "# Normalize + Compute Composite Score\n",
    "# -----------------------------------\n",
    "\n",
    "features = [\n",
    "    \"max_depth\",\n",
    "    \"avg_parent_distance\",\n",
    "    \"pct_cross_para\",\n",
    "    \"avg_candidate_count\"\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "complexity_df[features] = scaler.fit_transform(complexity_df[features])\n",
    "\n",
    "complexity_df[\"complexity_score\"] = complexity_df[features].mean(axis=1)\n",
    "\n",
    "complexity_df[\"complexity_level\"] = pd.qcut(\n",
    "    complexity_df[\"complexity_score\"],\n",
    "    q=3,\n",
    "    labels=[\"LOW\", \"MID\", \"HIGH\"]\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# Display\n",
    "# -----------------------------------\n",
    "\n",
    "display(complexity_df.head())\n",
    "print(\"\\nComplexity distribution:\")\n",
    "print(complexity_df[\"complexity_level\"].value_counts())\n",
    "complexity_df.to_csv(\"essay_complexity_metrics.csv\", index=False)\n",
    "print(\"Saved: essay_complexity_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafd298-9f89-4361-8a9f-49537cbb03fa",
   "metadata": {},
   "source": [
    "## Section 5: Parent–Child Structural Prediction with RoBERTa\n",
    "\n",
    "This section presents a binary classification model for predicting parent–child relationships between discourse segments in essays.\n",
    "\n",
    "The system performs:\n",
    "\n",
    "- Binary edge prediction (Is this candidate the true parent?)\n",
    "- Ranking-based parent selection (Select the highest-scoring parent per child)\n",
    "- Stratified evaluation by structural complexity (LOW/ MID/ HIGH)\n",
    "\n",
    "### 5.1 Model Overview\n",
    "\n",
    "We fine-tune RoBERTa-base for binary classification.\n",
    "\n",
    "Each input consists of:\n",
    "1. Structural features (e.g., segment distance, paragraph relation)\n",
    "2. The child segment text\n",
    "3. The candidate parent segment text\n",
    "\n",
    "These are combined into a single input sequence so the model can jointly reason over structural signals and semantic content.\n",
    "\n",
    "\n",
    "\n",
    "### 5.2 Training Setup\n",
    "\n",
    "- Maximum sequence length: 192 tokens\n",
    "- Batch size: 8  \n",
    "- Learning rate: 1e-5  \n",
    "- Number of epochs: 7  \n",
    "- Random seed: 42  (for reproducibility)\n",
    "\n",
    "To address class imbalance (many incorrect parent candidates vs. fewer correct ones), we use a weighted binary cross-entropy loss, which increases the penalty for misclassifying true parent links. \n",
    "\n",
    "\n",
    "### 5.3 Results \n",
    "To ensure reproducibility and transparency, training statistics were recovered from the saved model checkpoint (trainer_state.json). This includes epoch-level training loss, validation loss, and validation Macro F1 scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf34e731-7f54-4827-ad1d-d38e581aabcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Test essays by complexity:\n",
      "complexity_level\n",
      "LOW     2\n",
      "HIGH    2\n",
      "MID     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Essay IDs:\n",
      "['2008_2_1', '2008_3_2', '2021_1_2', '2021_5_2', '2021_7_1']\n",
      "\n",
      "Flat test pairs: 176\n",
      "Virtual test pairs: 324\n",
      "[best checkpoint] ./parent_results -> parent_results/checkpoint-1071\n",
      "[best checkpoint] ./parent_results_v2_virtual -> parent_results_v2_virtual/checkpoint-1953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2953b4da4e463d92c48d1a22cc65fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5273b9684449558cdcc24c5cefa1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6c6383ec0d43f083e7a79cc2263eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "FLAT (Collapsed) Model\n",
      "==============================\n",
      "TEST Macro F1: 0.6537\n",
      "Confusion Matrix:\n",
      "[[123  18]\n",
      " [ 20  15]]\n",
      "\n",
      "Parent Selection Accuracy (Ranking): 0.6286\n",
      "\n",
      "==============================\n",
      "Macro F1 by Complexity Level\n",
      "==============================\n",
      "LOW | Essays: 2 | Samples: 43 | Macro F1: 0.5968\n",
      "MID | Essays: 1 | Samples: 25 | Macro F1: 0.8252\n",
      "HIGH | Essays: 2 | Samples: 108 | Macro F1: 0.6318\n",
      "\n",
      "==============================\n",
      "Ranking Accuracy by Complexity Level\n",
      "==============================\n",
      "LOW | Essays: 2 | Ranking Accuracy: 0.9091\n",
      "MID | Essays: 1 | Ranking Accuracy: 1.0\n",
      "HIGH | Essays: 2 | Ranking Accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9fc6b022f44a399153796436595363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "VIRTUAL (Expanded) Model\n",
      "==============================\n",
      "TEST Macro F1: 0.5592\n",
      "Confusion Matrix:\n",
      "[[270   5]\n",
      " [ 43   6]]\n",
      "\n",
      "Parent Selection Accuracy (Ranking): 0.551\n",
      "\n",
      "==============================\n",
      "Macro F1 by Complexity Level\n",
      "==============================\n",
      "LOW | Essays: 2 | Samples: 64 | Macro F1: 0.4913\n",
      "MID | Essays: 1 | Samples: 51 | Macro F1: 0.4574\n",
      "HIGH | Essays: 2 | Samples: 209 | Macro F1: 0.6102\n",
      "\n",
      "==============================\n",
      "Ranking Accuracy by Complexity Level\n",
      "==============================\n",
      "LOW | Essays: 2 | Ranking Accuracy: 0.5714\n",
      "MID | Essays: 1 | Ranking Accuracy: 0.25\n",
      "HIGH | Essays: 2 | Ranking Accuracy: 0.6296\n",
      "\n",
      "Saved: Collapsed_bert_tree_comparison_test.csv\n",
      "Edge-level accuracy: 0.6286\n",
      "Compared edges: 35\n",
      "\n",
      "Saved: Expanded_bert_tree_comparison_test.csv\n",
      "Edge-level accuracy: 0.551\n",
      "Compared edges: 49\n",
      "\n",
      "==============================\n",
      "Collapsed Model – Sample Tree Comparisons\n",
      "==============================\n",
      "   essay_id  child_node_id  pred_parent_node_id  gold_parent_node_id  correct\n",
      "0  2008_2_1              3                    2                    2        1\n",
      "1  2008_2_1              4                    2                    2        1\n",
      "2  2008_2_1              5                    2                    2        1\n",
      "3  2008_2_1              6                    2                    2        1\n",
      "4  2008_2_1              7                    2                    2        1\n",
      "5  2008_2_1              8                    2                    2        1\n",
      "6  2008_2_1              9                    2                    8        0\n",
      "7  2008_2_1             12                   10                   10        1\n",
      "8  2008_3_2              3                    2                    2        1\n",
      "9  2008_3_2              4                    2                    2        1\n",
      "\n",
      "==============================\n",
      "Expanded Model – Sample Tree Comparisons\n",
      "==============================\n",
      "   essay_id  child_node_id  pred_parent_node_id  gold_parent_node_id  correct\n",
      "0  2008_2_1              3                    2                    2        1\n",
      "1  2008_2_1              4                    2                    2        1\n",
      "2  2008_2_1              5                    2                    2        1\n",
      "3  2008_2_1              6                    2                    2        1\n",
      "4  2008_2_1              7                    2                    2        1\n",
      "5  2008_2_1              8                    2                    2        1\n",
      "6  2008_2_1              9                    2                    8        0\n",
      "7  2008_2_1             12                    2                   10        0\n",
      "8  2008_3_2              3                   -2                    2        0\n",
      "9  2008_3_2              4                   -1                    2        0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "MAX_LEN = 192\n",
    "\n",
    "FLAT_DATA = \"dataset_for_parent_pairwise_models.csv\"\n",
    "VIRTUAL_DATA = \"dataset_parent_pairwise_v2_virtual.csv\"\n",
    "\n",
    "FLAT_OUTDIR = \"./parent_results\"                 # contains checkpoints + trainer_state.json\n",
    "VIRTUAL_OUTDIR = \"./parent_results_v2_virtual\"   # contains checkpoints + trainer_state.json\n",
    "\n",
    "GOLD_XLSX = \"FULL_ARGUMENT_STRUCTURE.xlsx\"\n",
    "\n",
    "\n",
    "DEVICE = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def load_best_checkpoint(output_dir: str) -> str:\n",
    "    \"\"\"Read best checkpoint path from trainer_state.json.\"\"\"\n",
    "    # trainer_state.json can exist at output_dir root OR inside checkpoints.\n",
    "    state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "    if not os.path.exists(state_path):\n",
    "        # fallback: find a trainer_state.json inside a checkpoint\n",
    "        ckpts = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        ckpts = sorted(ckpts, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(f\"No trainer_state.json or checkpoints found in {output_dir}\")\n",
    "        state_path = os.path.join(output_dir, ckpts[-1], \"trainer_state.json\")\n",
    "\n",
    "    with open(state_path, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "\n",
    "    best = state.get(\"best_model_checkpoint\", None)\n",
    "    if best is None:\n",
    "        # fallback: use the newest checkpoint\n",
    "        ckpts = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        ckpts = sorted(ckpts, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "        best = os.path.join(output_dir, ckpts[-1])\n",
    "\n",
    "    # best can be relative or absolute; normalize:\n",
    "    if not os.path.isabs(best):\n",
    "        # sometimes best already includes output_dir; sometimes it's like \"./parent_results/checkpoint-xxx\"\n",
    "        best = os.path.normpath(best)\n",
    "\n",
    "    print(f\"[best checkpoint] {output_dir} -> {best}\")\n",
    "    return best\n",
    "\n",
    "\n",
    "def build_complexity_split(seed=42):\n",
    "    \"\"\"Recreate essay-level stratified split exactly as training.\"\"\"\n",
    "    complexity_df = pd.read_csv(\"essay_complexity_metrics.csv\")\n",
    "\n",
    "    features = [\"max_depth\", \"avg_parent_distance\", \"pct_cross_para\", \"avg_candidate_count\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    complexity_df[features] = scaler.fit_transform(complexity_df[features])\n",
    "    complexity_df[\"complexity_score\"] = complexity_df[features].mean(axis=1)\n",
    "\n",
    "    complexity_df[\"complexity_level\"] = pd.qcut(\n",
    "        complexity_df[\"complexity_score\"], q=3, labels=[\"LOW\", \"MID\", \"HIGH\"]\n",
    "    )\n",
    "\n",
    "    essay_complexity = complexity_df[[\"essay_id\", \"complexity_level\"]].copy()\n",
    "\n",
    "    train_essays, temp_essays = train_test_split(\n",
    "        essay_complexity,\n",
    "        test_size=0.30,\n",
    "        random_state=seed,\n",
    "        stratify=essay_complexity[\"complexity_level\"]\n",
    "    )\n",
    "\n",
    "    val_essays, test_essays = train_test_split(\n",
    "        temp_essays,\n",
    "        test_size=0.50,\n",
    "        random_state=seed,\n",
    "        stratify=temp_essays[\"complexity_level\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nTest essays by complexity:\")\n",
    "    print(test_essays[\"complexity_level\"].value_counts())\n",
    "    print(\"\\nTest Essay IDs:\")\n",
    "    print(sorted(test_essays[\"essay_id\"].tolist()))\n",
    "\n",
    "    return essay_complexity, train_essays, val_essays, test_essays\n",
    "\n",
    "\n",
    "def tokenize_pairs(tokenizer, batch):\n",
    "    texts = []\n",
    "    for c, p, dist, same_p, para_d in zip(\n",
    "        batch[\"child_text\"], batch[\"parent_text\"],\n",
    "        batch[\"seg_distance\"], batch[\"same_para\"], batch[\"para_distance\"]\n",
    "    ):\n",
    "        prefix = f\"[DIST={dist}] [SAME_PARA={same_p}] [PARA_DIST={para_d}] \"\n",
    "        texts.append(prefix + f\"[CHILD] {c} [PARENT] {p}\")\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "    enc[\"labels\"] = batch[\"y\"]\n",
    "    return enc\n",
    "\n",
    "\n",
    "def predict_scores(model, tokenizer, test_df):\n",
    "    \"\"\"Return probs (score), pred_labels, and true labels for test pairs.\"\"\"\n",
    "    ds = Dataset.from_pandas(test_df)\n",
    "    ds = ds.map(lambda b: tokenize_pairs(tokenizer, b), batched=True)\n",
    "\n",
    "    cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    ds = ds.remove_columns([c for c in ds.column_names if c not in cols])\n",
    "    ds.set_format(\"torch\")\n",
    "\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ex in ds:\n",
    "            input_ids = ex[\"input_ids\"].unsqueeze(0).to(DEVICE)\n",
    "            attention_mask = ex[\"attention_mask\"].unsqueeze(0).to(DEVICE)\n",
    "            y = ex[\"labels\"].cpu().numpy().item()\n",
    "\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logit = out.logits.squeeze().detach().cpu().numpy().item()\n",
    "\n",
    "            all_logits.append(logit)\n",
    "            all_labels.append(y)\n",
    "\n",
    "    logits = np.array(all_logits)\n",
    "    labels = np.array(all_labels).astype(int)\n",
    "\n",
    "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    pred_labels = (probs > 0.5).astype(int)\n",
    "\n",
    "    return probs, pred_labels, labels\n",
    "\n",
    "\n",
    "def eval_all(test_df, probs, pred_labels, labels, essay_complexity, title=\"MODEL\"):\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(title)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    macro = f1_score(labels, pred_labels, average=\"macro\")\n",
    "    print(\"TEST Macro F1:\", round(macro, 4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(labels, pred_labels))\n",
    "\n",
    "    # Build eval DF\n",
    "    test_eval_df = test_df.copy()\n",
    "    test_eval_df[\"score\"] = probs\n",
    "    test_eval_df[\"pred\"] = pred_labels\n",
    "    test_eval_df[\"y\"] = labels\n",
    "\n",
    "    # Overall ranking accuracy (IMPORTANT: group by essay_id + child_node_id)\n",
    "    correct = []\n",
    "    for (essay_id, child_id), g in test_eval_df.groupby([\"essay_id\", \"child_node_id\"]):\n",
    "        best = g.loc[g[\"score\"].idxmax()]\n",
    "        correct.append(best[\"y\"])\n",
    "    rank_acc = float(np.mean(correct))\n",
    "    print(\"\\nParent Selection Accuracy (Ranking):\", round(rank_acc, 4))\n",
    "\n",
    "    # Merge complexity + by-level metrics\n",
    "    test_eval_df = test_eval_df.merge(essay_complexity, on=\"essay_id\", how=\"left\")\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Macro F1 by Complexity Level\")\n",
    "    print(\"==============================\")\n",
    "    for lvl in [\"LOW\", \"MID\", \"HIGH\"]:\n",
    "        sub = test_eval_df[test_eval_df[\"complexity_level\"] == lvl]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        f1 = f1_score(sub[\"y\"], sub[\"pred\"], average=\"macro\")\n",
    "        print(lvl,\n",
    "              \"| Essays:\", sub[\"essay_id\"].nunique(),\n",
    "              \"| Samples:\", len(sub),\n",
    "              \"| Macro F1:\", round(f1, 4))\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Ranking Accuracy by Complexity Level\")\n",
    "    print(\"==============================\")\n",
    "    for lvl in [\"LOW\", \"MID\", \"HIGH\"]:\n",
    "        sub = test_eval_df[test_eval_df[\"complexity_level\"] == lvl]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        c = []\n",
    "        for (essay_id, child_id), g in sub.groupby([\"essay_id\", \"child_node_id\"]):\n",
    "            best = g.loc[g[\"score\"].idxmax()]\n",
    "            c.append(best[\"y\"])\n",
    "        print(lvl,\n",
    "              \"| Essays:\", sub[\"essay_id\"].nunique(),\n",
    "              \"| Ranking Accuracy:\", round(float(np.mean(c)), 4))\n",
    "\n",
    "    return test_eval_df\n",
    "\n",
    "\n",
    "def tree_compare_from_test_eval(test_eval_df, out_csv, gold_xlsx=GOLD_XLSX):\n",
    "    # predicted tree edges\n",
    "    pred_rows = []\n",
    "    for (essay_id, child_id), g in test_eval_df.groupby([\"essay_id\", \"child_node_id\"]):\n",
    "        best = g.loc[g[\"score\"].idxmax()]\n",
    "        pred_rows.append({\n",
    "            \"essay_id\": str(essay_id),\n",
    "            \"child_node_id\": int(child_id),\n",
    "            \"pred_parent_node_id\": int(best[\"cand_parent_node_id\"])\n",
    "        })\n",
    "    pred_tree = pd.DataFrame(pred_rows)\n",
    "\n",
    "    # gold tree edges\n",
    "    structure_df = pd.read_excel(gold_xlsx)\n",
    "    gold_tree = structure_df[[\"essay_id\", \"node_id\", \"parent_node_id\"]].rename(columns={\n",
    "        \"node_id\": \"child_node_id\",\n",
    "        \"parent_node_id\": \"gold_parent_node_id\"\n",
    "    }).dropna(subset=[\"gold_parent_node_id\"])\n",
    "\n",
    "    gold_tree[\"essay_id\"] = gold_tree[\"essay_id\"].astype(str)\n",
    "    gold_tree[\"child_node_id\"] = gold_tree[\"child_node_id\"].astype(int)\n",
    "    gold_tree[\"gold_parent_node_id\"] = gold_tree[\"gold_parent_node_id\"].astype(int)\n",
    "\n",
    "    pred_tree[\"essay_id\"] = pred_tree[\"essay_id\"].astype(str)\n",
    "\n",
    "    comp = pred_tree.merge(gold_tree, on=[\"essay_id\", \"child_node_id\"], how=\"inner\")\n",
    "    comp[\"correct\"] = (comp[\"pred_parent_node_id\"] == comp[\"gold_parent_node_id\"]).astype(int)\n",
    "\n",
    "    edge_acc = comp[\"correct\"].mean()\n",
    "    print(f\"\\nSaved: {out_csv}\")\n",
    "    print(\"Edge-level accuracy:\", round(float(edge_acc), 4))\n",
    "    print(\"Compared edges:\", len(comp))\n",
    "\n",
    "    comp.to_csv(out_csv, index=False)\n",
    "    return comp\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Recreate split ONCE (used by both datasets)\n",
    "# -----------------------------\n",
    "essay_complexity, train_essays, val_essays, test_essays = build_complexity_split(SEED)\n",
    "train_ids = set(train_essays[\"essay_id\"].tolist())\n",
    "val_ids = set(val_essays[\"essay_id\"].tolist())\n",
    "test_ids = set(test_essays[\"essay_id\"].tolist())\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load datasets + build test_dfs (flat + virtual)\n",
    "# -----------------------------\n",
    "flat_df = pd.read_csv(FLAT_DATA).dropna(subset=[\"child_text\", \"parent_text\", \"y\"])\n",
    "virtual_df = pd.read_csv(VIRTUAL_DATA).dropna(subset=[\"child_text\", \"parent_text\", \"y\"])\n",
    "\n",
    "flat_test_df = flat_df[flat_df[\"essay_id\"].isin(test_ids)].copy()\n",
    "virtual_test_df = virtual_df[virtual_df[\"essay_id\"].isin(test_ids)].copy()\n",
    "\n",
    "print(\"\\nFlat test pairs:\", len(flat_test_df))\n",
    "print(\"Virtual test pairs:\", len(virtual_test_df))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load best checkpoints automatically\n",
    "# -----------------------------\n",
    "flat_ckpt = load_best_checkpoint(FLAT_OUTDIR)\n",
    "virtual_ckpt = load_best_checkpoint(VIRTUAL_OUTDIR)\n",
    "\n",
    "flat_tokenizer = AutoTokenizer.from_pretrained(flat_ckpt)\n",
    "flat_model = AutoModelForSequenceClassification.from_pretrained(flat_ckpt, num_labels=1).to(DEVICE)\n",
    "\n",
    "virtual_tokenizer = AutoTokenizer.from_pretrained(virtual_ckpt)\n",
    "virtual_model = AutoModelForSequenceClassification.from_pretrained(virtual_ckpt, num_labels=1).to(DEVICE)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Predict + Evaluate (flat + virtual)\n",
    "# -----------------------------\n",
    "flat_probs, flat_pred, flat_y = predict_scores(flat_model, flat_tokenizer, flat_test_df)\n",
    "flat_test_eval = eval_all(flat_test_df, flat_probs, flat_pred, flat_y, essay_complexity, title=\"FLAT (Collapsed) Model\")\n",
    "\n",
    "virtual_probs, virtual_pred, virtual_y = predict_scores(virtual_model, virtual_tokenizer, virtual_test_df)\n",
    "virtual_test_eval = eval_all(virtual_test_df, virtual_probs, virtual_pred, virtual_y, essay_complexity, title=\"VIRTUAL (Expanded) Model\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Tree comparisons (flat + virtual)\n",
    "# -----------------------------\n",
    "flat_tree_comp = tree_compare_from_test_eval(flat_test_eval, \"Collapsed_bert_tree_comparison_test.csv\")\n",
    "virtual_tree_comp = tree_compare_from_test_eval(virtual_test_eval, \"Expanded_bert_tree_comparison_test.csv\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Collapsed Model – Sample Tree Comparisons\")\n",
    "print(\"==============================\")\n",
    "print(flat_tree_comp.head(10))\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Expanded Model – Sample Tree Comparisons\")\n",
    "print(\"==============================\")\n",
    "print(virtual_tree_comp.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
